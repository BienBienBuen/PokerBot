
1

Automatic Zoom
Deep Counterfactual Regret Minimization
Noam Brown * 1 2 Adam Lerer * 2 Sam Gross 2 Tuomas Sandholm 1
Abstract
Counterfactual Regret Minimization (CFR) is the
leading framework for solving large imperfect-
information games. It converges to an equilibrium
by iteratively traversing the game tree. In order
to deal with extremely large games, abstraction
is typically applied before running CFR. The ab-
stracted game is solved with tabular CFR, and its
solution is mapped back to the full game. This
process can be problematic because aspects of
abstraction are often manual and domain specific,
abstraction algorithms may miss important strate-
gic nuances of the game, and there is a chicken-
and-egg problem because determining a good ab-
straction requires knowledge of the equilibrium
of the game. This paper introduces Deep Counter-
factual Regret Minimization, a form of CFR that
obviates the need for abstraction by instead using
deep neural networks to approximate the behavior
of CFR in the full game. We show that Deep CFR
is principled and achieves strong performance in
large poker games. This is the first non-tabular
variant of CFR to be successful in large games.
1. Introduction
Imperfect-information games model strategic interactions
between multiple agents with only partial information. They
are widely applicable to real-world domains such as negoti-
ations, auctions, and cybersecurity interactions. Typically
in such games, one wishes to find an approximate equilib-
rium in which no player can improve by deviating from the
equilibrium.
The most successful family of algorithms for imperfect-
information games have been variants of Counterfactual
Regret Minimization (CFR) (Zinkevich et al., 2007). CFR is
an iterative algorithm that converges to a Nash equilibrium
in two-player zero-sum games. Forms of tabular CFR have
* Equal contribution 1 Computer Science Department, Carnegie
Mellon University 2 Facebook AI Research. Correspondence to:
Noam Brown <noamb@cs.cmu.edu>.
Proceedings of the 36th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).
been used in all recent milestones in the benchmark domain
of poker (Bowling et al., 2015; Moravˇc ́ık et al., 2017; Brown
& Sandholm, 2017) and have been used in all competitive
agents in the Annual Computer Poker Competition going
back at least six years.1 In order to deal with extremely
large imperfect-information games, abstraction is typically
used to simplify a game by bucketing similar states together
and treating them identically. The simplified (abstracted)
game is approximately solved via tabular CFR. However,
constructing an effective abstraction requires extensive do-
main knowledge and the abstract solution may only be a
coarse approximation of a true equilibrium.
In constrast, reinforcement learning has been successfully
extended to large state spaces by using function approx-
imation with deep neural networks rather than a tabular
representation of the policy (deep RL). This approach has
led to a number of recent breakthroughs in constructing
strategies in large MDPs (Mnih et al., 2015) as well as in
zero-sum perfect-information games such as Go (Silver
et al., 2017; 2018).2 Importantly, deep RL can learn good
strategies with relatively little domain knowledge for the
specific game (Silver et al., 2017). However, most popular
RL algorithms do not converge to good policies (equilibria)
in imperfect-information games in theory or in practice.
Rather than use tabular CFR with abstraction, this paper
introduces a form of CFR, which we refer to as Deep Coun-
terfactual Regret Minimization, that uses function approx-
imation with deep neural networks to approximate the be-
havior of tabular CFR on the full, unabstracted game. We
prove that Deep CFR converges to an -Nash equilibrium
in two-player zero-sum games and empirically evaluate per-
formance in poker variants, including heads-up limit Texas
hold’em. We show Deep CFR outperforms Neural Ficti-
tious Self Play (NFSP) (Heinrich & Silver, 2016), which
was the prior leading function approximation algorithm for
imperfect-information games, and that Deep CFR is com-
petitive with domain-specific tabular abstraction techniques.
1 www.computerpokercompetition.org2 Deep RL has also been applied successfully to some partially
observed games such as Doom (Lample & Chaplot, 2017), as long
as the hidden information is not too strategically important.
arXiv:1811.00164v3  [cs.AI]  22 May 2019
Deep Counterfactual Regret Minimization
2. Notation and Background
In an imperfect-information extensive-form (that is, tree-
form) game there is a finite set of players, P. A node
(or history) h is defined by all information of the current
situation, including private knowledge known to only one
player. A(h) denotes the actions available at a node and
P(h) is either chance or the unique player who acts at that
node. If action a ∈A(h) leads from h to h′, then we write
h ·a = h′. We write h @h′ if a sequence of actions leads
from h to h′. H is the set of all nodes. Z ⊆H are terminal
nodes for which no actions are available. For each player
p ∈ P, there is a payoff function up : Z → R. In this
paper we assume P = {1,2}and u1 = −u2 (the game is
two-player zero-sum). We denote the range of payoffs in
the game by ∆.
Imperfect information is represented by information sets
(infosets) for each player p ∈ P. For any infoset I be-
longing to p, all nodes h,h′ ∈ I are indistinguishable to
p. Moreover, every non-terminal node h ∈ H belongs to
exactly one infoset for each p. We represent the set of all
infosets belonging to p where p acts by Ip. We call the
set of all terminal nodes with a prefix in I as ZI, and we
call the particular prefix z[I]. We assume the game features
perfect recall, which means if h and h′ do not share a player
p infoset then all nodes following h do not share a player p
infoset with any node following h′.
A strategy (or policy) σ(I) is a probability vector over ac-
tions for acting player p in infoset I. Since all states in an
infoset belonging to p are indistinguishable, the strategies
in each of them must be identical. The set of actions in I is
denoted by A(I). The probability of a particular action a is
denoted by σ(I,a). We define σp to be a strategy for p in
every infoset in the game where p acts. A strategy profile σ
is a tuple of strategies, one for each player. The strategy of
every player other than p is represented as σ−p. up(σp,σ−p)
is the expected payoff for p if player p plays according to
σp and the other players play according to σ−p.
πσ(h) = Πh′·avhσP(h′)(h′,a) is called reach and is the
probability h is reached if all players play according to σ.
πσp (h) is the contribution of p to this probability. πσ−p(h)
is the contribution of chance and all players other than p.
For an infoset I belonging to p, the probability of reaching
I if p chooses actions leading toward I but chance and all
players other than p play according to σ−p is denoted by
πσ−p(I) = ∑
h∈I πσ−p(h). For h vz, define πσ(h →z) =
Πh′·avz,h′6@hσP(h′)(h′,a)
A best response to σ−p is a player p strategy BR(σ−p)
such that up
(BR(σ−p),σ−p
) = maxσ′p up(σ′p,σ−p). A
Nash equilibrium σ∗ is a strategy profile where ev-
eryone plays a best response: ∀p, up(σ∗p,σ∗−p) =
maxσ′p up(σ′p,σ∗−p) (Nash, 1950). The exploitability e(σp)
of a strategy σp in a two-player zero-sum game is how much
worse σp does versus BR(σp) compared to how a Nash
equilibrium strategy σ∗p does against BR(σ∗p). Formally,
e(σp) = up
(σ∗p,BR(σ∗p)) −up
(σp,BR(σp)). We mea-
sure total exploitability ∑
p∈P e(σp)3.
2.1. Counterfactual Regret Minimization (CFR)
CFR is an iterative algorithm that converges to a Nash equi-
librium in any finite two-player zero-sum game with a the-
oretical convergence bound of O( 1√T ). In practice CFR
converges much faster. We provide an overview of CFR be-
low; for a full treatment, see Zinkevich et al. (2007). Some
recent forms of CFR converge in O( 1
T0.75 ) in self-play set-
tings (Farina et al., 2019), but are slower in practice so we
do not use them in this paper.
Let σt be the strategy profile on iteration t. The counter-
factual value vσ(I) of player p = P(I) at I is the expected
payoff to p when reaching I, weighted by the probability
that p would reached I if she tried to do so that iteration.
Formally,
vσ(I) = ∑
z∈ZI
πσ−p(z[I])πσ(z[I] →z)up(z) (1)
and vσ(I,a) is the same except it assumes that player p
plays action a at infoset I with 100% probability.
The instantaneous regret rt(I,a) is the difference between
P(I)’s counterfactual value from playing a vs. playing σ
on iteration t
rt(I,a) = vσt
(I,a) −vσt
(I) (2)
The counterfactual regret for infoset I action a on iteration
T is
RT(I,a) =
T∑
t=1
rt(I,a) (3)
Additionally, RT+(I,a) = max{RT(I,a),0}and RT(I) =
maxa{RT(I,a)}. Total regret for p in the entire game is
RTp = maxσ′p
∑T
t=1
(up(σ′p,σt−p) −up(σtp,σt−p)).
CFR determines an iteration’s strategy by applying any of
several regret minimization algorithms to each infoset (Lit-
tlestone & Warmuth, 1994; Chaudhuri et al., 2009). Typi-
cally, regret matching (RM) is used as the regret minimiza-
tion algorithm within CFR due to RM’s simplicity and lack
of parameters (Hart & Mas-Colell, 2000).
In RM, a player picks a distribution over actions in an in-
foset in proportion to the positive regret on those actions.
Formally, on each iteration t+ 1, p selects actions a ∈A(I)
3 Some prior papers instead measure average exploitability
rather than total (summed) exploitability.
Deep Counterfactual Regret Minimization
according to probabilities
σt+1(I,a) = Rt+(I,a)∑
a′∈A(I) Rt+(I,a′) (4)
If ∑
a′∈A(I) Rt+(I,a′) = 0 then any arbitrary strategy may
be chosen. Typically each action is assigned equal proba-
bility, but in this paper we choose the action with highest
counterfactual regret with probability 1, which we find em-
pirically helps RM better cope with approximation error
(see Figure 4).
If a player plays according to regret matching in in-
foset I on every iteration, then on iteration T, RT(I) ≤
∆√|A(I)|√T (Cesa-Bianchi & Lugosi, 2006). Zinkevich
et al. (2007) show that the sum of the counterfactual regret
across all infosets upper bounds the total regret. Therefore,
if player p plays according to CFR on every iteration, then
RTp ≤∑
I∈Ip RT(I). So, as T →∞, RT
p
T →0.
The average strategy  ̄σTp (I) for an infoset I on iteration T
is  ̄σTp (I) =
∑T
t=1
(πσt
p (I)σt
p(I)
)
∑T
t=1 πσt
p (I) .
In two-player zero-sum games, if both players’ average
total regret satisfies RT
p
T ≤ , then their average strategies
〈 ̄σT1 ,  ̄σT2 〉form a 2-Nash equilibrium (Waugh, 2009). Thus,
CFR constitutes an anytime algorithm for finding an -Nash
equilibrium in two-player zero-sum games.
In practice, faster convergence is achieved by alternating
which player updates their regrets on each iteration rather
than updating the regrets of both players simultaneously
each iteration, though this complicates the theory (Farina
et al., 2018; Burch et al., 2018). We use the alternating-
updates form of CFR in this paper.
2.2. Monte Carlo Counterfactual Regret Minimization
Vanilla CFR requires full traversals of the game tree, which
is infeasible in large games. One method to combat this is
Monte Carlo CFR (MCCFR), in which only a portion of
the game tree is traversed on each iteration (Lanctot et al.,
2009). In MCCFR, a subset of nodes Qt in the game tree is
traversed at each iteration, where Qt is sampled from some
distribution Q. Sampled regrets  ̃rt are tracked rather than
exact regrets. For infosets that are sampled at iteration t,
 ̃rt(I,a) is equal to rt(I,a) divided by the probability of
having sampled I; for unsampled infosets  ̃rt(I,a) = 0. See
Appendix B for more details.
There exist a number of MCCFR variants (Gibson et al.,
2012; Johanson et al., 2012; Jackson, 2017), but for this
paper we focus specifically on the external sampling variant
due to its simplicity and strong performance. In external-
sampling MCCFR the game tree is traversed for one player
at a time, alternating back and forth. We refer to the player
who is traversing the game tree on the iteration as the tra-
verser. Regrets are updated only for the traverser on an
iteration. At infosets where the traverser acts, all actions are
explored. At other infosets and chance nodes, only a single
action is explored.
External-sampling MCCFR probabilistically converges to
an equilibrium. For any ρ ∈(0,1], total regret is bounded
by RTp ≤(1 + √2√ρ
)|Ip|∆√|A|√T with probability 1 −ρ.
3. Related Work
CFR is not the only iterative algorithm capable of solving
large imperfect-information games. First-order methods
converge to a Nash equilibrium in O(1/T) (Hoda et al.,
2010; Kroer et al., 2018b;a), which is far better than CFR’s
theoretical bound. However, in practice the fastest variants
of CFR are substantially faster than the best first-order meth-
ods. Moreover, CFR is more robust to error and therefore
likely to do better when combined with function approxima-
tion.
Neural Fictitious Self Play (NFSP) (Heinrich & Silver,
2016) previously combined deep learning function approx-
imation with Fictitious Play (Brown, 1951) to produce an
AI for heads-up limit Texas hold’em, a large imperfect-
information game. However, Fictitious Play has weaker
theoretical convergence guarantees than CFR, and in prac-
tice converges slower. We compare our algorithm to NFSP
in this paper. Model-free policy gradient algorithms have
been shown to minimize regret when parameters are tuned
appropriately (Srinivasan et al., 2018) and achieve perfor-
mance comparable to NFSP.
Past work has investigated using deep learning to esti-
mate values at the depth limit of a subgame in imperfect-
information games (Moravˇc ́ık et al., 2017; Brown et al.,
2018). However, tabular CFR was used within the sub-
games themselves. Large-scale function approximated CFR
has also been developed for single-agent settings (Jin et al.,
2017). Our algorithm is intended for the multi-agent set-
ting and is very different from the one proposed for the
single-agent setting.
Prior work has combined regression tree function approxi-
mation with CFR (Waugh et al., 2015) in an algorithm called
Regression CFR (RCFR). This algorithm defines a number
of features of the infosets in a game and calculates weights
to approximate the regrets that a tabular CFR implemen-
tation would produce. Regression CFR is algorithmically
similar to Deep CFR, but uses hand-crafted features similar
to those used in abstraction, rather than learning the features.
RCFR also uses full traversals of the game tree (which is
infeasible in large games) and has only been evaluated on
toy games. It is therefore best viewed as the first proof of
concept that function approximation can be applied to CFR.
Deep Counterfactual Regret Minimization
Concurrent work has also investigated a similar combina-
tion of deep learning with CFR, in an algorithm referred
to as Double Neural CFR (Li et al., 2018). However, that
approach may not be theoretically sound and the authors
consider only small games. There are important differences
between our approaches in how training data is collected
and how the behavior of CFR is approximated.
4. Description of the Deep Counterfactual
Regret Minimization Algorithm
In this section we describe Deep CFR. The goal of Deep
CFR is to approximate the behavior of CFR without calcu-
lating and accumulating regrets at each infoset, by general-
izing across similar infosets using function approximation
via deep neural networks.
On each iteration t, Deep CFR conducts a constant num-
ber K of partial traversals of the game tree, with the path
of the traversal determined according to external sampling
MCCFR. At each infoset I it encounters, it plays a strategy
σt(I) determined by regret matching on the output of a neu-
ral network V : I →R|A| defined by parameters θt−1p that
takes as input the infoset I and outputs values V (I,a|θt−1).
Our goal is for V (I,a|θt−1) to be approximately propor-
tional to the regret Rt−1(I,a) that tabular CFR would have
produced.
When a terminal node is reached, the value is passed back up.
In chance and opponent infosets, the value of the sampled
action is passed back up unaltered. In traverser infosets, the
value passed back up is the weighted average of all action
values, where action a’s weight is σt(I,a). This produces
samples of this iteration’s instantaneous regrets for various
actions. Samples are added to a memory Mv,p, where p
is the traverser, using reservoir sampling (Vitter, 1985) if
capacity is exceeded.
Consider a nice property of the sampled instantaneous re-
grets induced by external sampling:
Lemma 1. For external sampling MCCFR, the sampled
instantaneous regrets are an unbiased estimator of the ad-
vantage, i.e. the difference in expected payoff for playing
a vs σtp(I) at I, assuming both players play σt everywhere
else.
EQ∈Qt
[
 ̃rσt
p (I,a)
∣∣∣ZI ∩Q 6= ∅
]
= vσt
(I,a) −vσt
(I)
πσt
−p(I) .
The proof is provided in Appendix B.2.
Recent work in deep reinforcement learning has shown
that neural networks can effectively predict and generalize
advantages in challenging environments with large state
spaces, and use that to learn good policies (Mnih et al.,
2016).
Once a player’s K traversals are completed, a new network
is trained from scratch to determine parameters θtp by mini-
mizing MSE between predicted advantage Vp(I,a|θt) and
samples of instantaneous regrets from prior iterations t′ ≤t
 ̃rt′
(I,a) drawn from the memory. The average over all
sampled instantaneous advantages  ̃rt′
(I,a) is proportional
to the total sampled regret  ̃Rt(I,a) (across actions in an
infoset), so once a sample is added to the memory it is never
removed except through reservoir sampling, even when the
next CFR iteration begins.
One can use any loss function for the value and average
strategy model that satisfies Bregman divergence (Banerjee
et al., 2005), such as mean squared error loss.
While almost any sampling scheme is acceptable so long
as the samples are weighed properly, external sampling
has the convenient property that it achieves both of our
desired goals by assigning all samples in an iteration equal
weight. Additionally, exploring all of a traverser’s actions
helps reduce variance. However, external sampling may
be impractical in games with extremely large branching
factors, so a different sampling scheme, such as outcome
sampling (Lanctot et al., 2009), may be desired in those
cases.
In addition to the value network, a separate policy network
Π : I →R|A| approximates the average strategy at the end
of the run, because it is the average strategy played over all
iterations that converges to a Nash equilibrium. To do this,
we maintain a separate memory MΠ of sampled infoset
probability vectors for both players. Whenever an infoset
I belonging to player p is traversed during the opposing
player’s traversal of the game tree via external sampling,
the infoset probability vector σt(I) is added to MΠ and
assigned weight t.
If the number of Deep CFR iterations and the size of each
value network model is small, then one can avoid training
the final policy network by instead storing each iteration’s
value network (Steinberger, 2019). During actual play, a
value network is sampled randomly and the player plays the
CFR strategy resulting from the predicted advantages of that
network. This eliminates the function approximation error
of the final average policy network, but requires storing all
prior value networks. Nevertheless, strong performance and
low exploitability may still be achieved by storing only a
subset of the prior value networks (Jackson, 2016).
Theorem 1 states that if the memory buffer is sufficiently
large, then with high probability Deep CFR will result in
average regret being bounded by a constant proportional to
the square root of the function approximation error.
Theorem 1. Let T denote the number of Deep CFR itera-
tions, |A| the maximum number of actions at any infoset,
and K the number of traversals per iteration. Let LtV be
Deep Counterfactual Regret Minimization
the average MSE loss for Vp(I,a|θt) on a sample in MV,p
at iteration t , and let LtV ∗ be the minimum loss achievable
for any function V . Let LtV −LtV ∗ ≤L.
If the value memories are sufficiently large, then with proba-
bility 1 −ρ total regret at time T is bounded by
RTp ≤
(
1 +
√2√ρK
)
∆|Ip|√|A|√T + 4T|Ip|√|A|∆L
(5)
with probability 1 −ρ.
Corollary 1. As T →∞, average regret RT
p
T is bounded by
4|Ip|√|A|∆L
with high probability.
The proofs are provided in Appendix B.4.
We do not provide a convergence bound for Deep CFR when
using linear weighting, since the convergence rate of Linear
CFR has not been shown in the Monte Carlo case. However,
Figure 4 shows moderately faster convergence in practice.
5. Experimental Setup
We measure the performance of Deep CFR (Algorithm 1)
in approximating an equilibrium in heads-up flop hold’em
poker (FHP). FHP is a large game with over 1012 nodes
and over 109 infosets. In contrast, the network we use has
98,948 parameters. FHP is similar to heads-up limit Texas
hold’em (HULH) poker, but ends after the second betting
round rather than the fourth, with only three community
cards ever dealt. We also measure performance relative to
domain-specific abstraction techniques in the benchmark
domain of HULH poker, which has over 1017 nodes and
over 1014 infosets. The rules for FHP and HULH are given
in Appendix A.
In both games, we compare performance to NFSP, which
is the previous leading algorithm for imperfect-information
game solving using domain-independent function approx-
imation, as well as state-of-the-art abstraction techniques
designed for the domain of poker (Johanson et al., 2013;
Ganzfried & Sandholm, 2014; Brown et al., 2015).
5.1. Network Architecture
We use the neural network architecture shown in Figure 5.1
for both the value network V that computes advantages for
each player and the network Π that approximates the final
average strategy. This network has a depth of 7 layers and
98,948 parameters. Infosets consist of sets of cards and
bet history. The cards are represented as the sum of three
embeddings: a rank embedding (1-13), a suit embedding
Figure 1. The neural network architecture used for Deep CFR.
The network takes an infoset (observed cards and bet history) as
input and outputs values (advantages or probability logits) for each
possible action.
(1-4), and a card embedding (1-52). These embeddings
are summed for each set of permutation invariant cards
(hole, flop, turn, river), and these are concatenated. In
each of the Nrounds rounds of betting there can be at most 6
sequential actions, leading to 6Nrounds total unique betting
positions. Each betting position is encoded by a binary
value specifying whether a bet has occurred, and a float
value specifying the bet size.
The neural network model begins with separate branches for
the cards and bets, with three and two layers respectively.
Features from the two branches are combined and three
additional fully connected layers are applied. Each fully-
connected layer consists of xi+1 = ReLU(Ax[+x]). The
optional skip connection [+x] is applied only on layers that
have equal input and output dimension. Normalization (to
zero mean and unit variance) is applied to the last-layer
features. The network architecture was not highly tuned, but
normalization and skip connections were used because they
were found to be important to encourage fast convergence
when running preliminary experiments on pre-computed
equilibrium strategies in FHP. A full network specification
is provided in Appendix C.
In the value network, the vector of outputs represented pre-
dicted advantages for each action at the input infoset. In the
average strategy network, outputs are interpreted as logits
of the probability distribution over actions.
5.2. Model training
We allocate a maximum size of 40 million infosets to each
player’s advantage memory MV,p and the strategy memory
MΠ. The value model is trained from scratch each CFR
iteration, starting from a random initialization. We perform
4,000 mini-batch stochastic gradient descent (SGD) itera-
tions using a batch size of 10,000 and perform parameter
updates using the Adam optimizer (Kingma & Ba, 2014)
with a learning rate of 0.001, with gradient norm clipping
to 1. For HULH we use 32,000 SGD iterations and a batch
size of 20,000. Figure 4 shows that training the model from
Deep Counterfactual Regret Minimization
Algorithm 1 Deep Counterfactual Regret Minimization
function DEEPCFR
Initialize each player’s advantage network V (I,a|θp) with parameters θp so that it returns 0 for all inputs.
Initialize reservoir-sampled advantage memories MV,1,MV,2 and strategy memory MΠ.
for CFR iteration t = 1 to T do
for each player p do
for traversal k = 1 to K do
TRAVERSE(∅,p,θ1,θ2,MV,p,MΠ) . Collect data from a game traversal with external sampling
Train θp from scratch on loss L(θp) = E(I,t′, ̃rt′)∼MV,p
[
t′∑
a
(
 ̃rt′
(a) −V (I,a|θp)
)2]
Train θΠ on loss L(θΠ) = E(I,t′,σt′)∼MΠ
[
t′∑
a
(
σt′
(a) −Π(I,a|θΠ)
)2]
return θΠ
Algorithm 2 CFR Traversal with External Sampling
function TRAVERSE(h,p,θ1,θ2,MV ,MΠ, t)
Input: History h, traverser player p, regret network parameters θ for each player, advantage memory MV for player
p, strategy memory MΠ, CFR iteration t.
if h is terminal then
return the payoff to player p
else if h is a chance node then
a ∼σ(h)
return TRAVERSE(h ·a,p,θ1,θ2,MV ,MΠ, t)
else if P(h) = p then . If it’s the traverser’s turn to act
Compute strategy σt(I) from predicted advantages V (I(h),a|θp) using regret matching.
for a ∈A(h) do
v(a) ←TRAVERSE(h ·a,p,θ1,θ2,MV ,MΠ, t) . Traverse each action
for a ∈A(h) do
 ̃r(I,a) ←v(a) −∑
a′∈A(h) σ(I,a′) ·v(a′) . Compute advantages
Insert the infoset and its action advantages (I,t,  ̃rt(I)) into the advantage memory MV
else . If it’s the opponent’s turn to act
Compute strategy σt(I) from predicted advantages V (I(h),a|θ3−p) using regret matching.
Insert the infoset and its action probabilities (I,t,σt(I)) into the strategy memory MΠ
Sample an action a from the probability distribution σt(I).
return TRAVERSE(h ·a,p,θ1,θ2,MV ,MΠ, t)
scratch at each iteration, rather than using the weights from
the previous iteration, leads to better convergence.
5.3. Linear CFR
There exist a number of variants of CFR that achieve much
faster performance than vanilla CFR. However, most of
these faster variants of CFR do not handle approximation
error well (Tammelin et al., 2015; Burch, 2017; Brown &
Sandholm, 2019; Schmid et al., 2019). In this paper we use
Linear CFR (LCFR) (Brown & Sandholm, 2019), a variant
of CFR that is faster than CFR and in certain settings is
the fastest-known variant of CFR (particularly in settings
with wide distributions in payoffs), and which tolerates
approximation error well. LCFR is not essential and does
not appear to lead to better performance asymptotically, but
does result in faster convergence in our experiments.
LCFR is like CFR except iteration t is weighed by t. Specif-
ically, we maintain a weight on each entry stored in the
advantage memory and the strategy memory, equal to t
when this entry was added. When training θp each itera-
tion T, we rescale all the batch weights by 2
T and minimize
weighted error.
6. Experimental Results
Figure 2 compares the performance of Deep CFR to
different-sized domain-specific abstractions in FHP. The ab-
stractions are solved using external-sampling Linear Monte
Deep Counterfactual Regret Minimization
References
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. Clus-
tering with bregman divergences. Journal of machine
learning research, 6(Oct):1705–1749, 2005.
Bowling, M., Burch, N., Johanson, M., and Tammelin, O.
Heads-up limit hold’em poker is solved. Science, 347
(6218):145–149, January 2015.
Brown, G. W. Iterative solutions of games by fictitious play.
In Koopmans, T. C. (ed.), Activity Analysis of Production
and Allocation, pp. 374–376. John Wiley & Sons, 1951.
Brown, N. and Sandholm, T. Superhuman AI for heads-up
no-limit poker: Libratus beats top professionals. Science,
pp. eaao1733, 2017.
Brown, N. and Sandholm, T. Solving imperfect-information
games via discounted regret minimization. In AAAI Con-
ference on Artificial Intelligence (AAAI), 2019.
Brown, N., Ganzfried, S., and Sandholm, T. Hierarchical ab-
straction, distributed equilibrium computation, and post-
processing, with application to a champion no-limit texas
hold’em agent. In Proceedings of the 2015 International
Conference on Autonomous Agents and Multiagent Sys-
tems, pp. 7–15. International Foundation for Autonomous
Agents and Multiagent Systems, 2015.
Brown, N., Sandholm, T., and Amos, B. Depth-limited
solving for imperfect-information games. In Advances in
Neural Information Processing Systems, 2018.
Burch, N. Time and Space: Why Imperfect Information
Games are Hard. PhD thesis, University of Alberta, 2017.
Burch, N., Moravcik, M., and Schmid, M. Revisiting cfr+
and alternating updates. arXiv preprint arXiv:1810.11542,
2018.
Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and
games. Cambridge University Press, 2006.
Chaudhuri, K., Freund, Y., and Hsu, D. J. A parameter-free
hedging algorithm. In Advances in neural information
processing systems, pp. 297–305, 2009.
Farina, G., Kroer, C., and Sandholm, T. Online convex opti-
mization for sequential decision processes and extensive-
form games. In AAAI Conference on Artificial Intelli-
gence (AAAI), 2018.
Farina, G., Kroer, C., Brown, N., and Sandholm, T. Stable-
predictive optimistic counterfactual regret minimization.
In International Conference on Machine Learning, 2019.
Ganzfried, S. and Sandholm, T. Potential-aware imperfect-
recall abstraction with earth mover’s distance in
imperfect-information games. In AAAI Conference on
Artificial Intelligence (AAAI), 2014.
Gibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowl-
ing, M. Generalized sampling and variance in coun-
terfactual regret minimization. In Proceedins of the
Twenty-Sixth AAAI Conference on Artificial Intelligence,
pp. 1355–1361, 2012.
Hart, S. and Mas-Colell, A. A simple adaptive procedure
leading to correlated equilibrium. Econometrica, 68:
1127–1150, 2000.
Heinrich, J. and Silver, D. Deep reinforcement learning
from self-play in imperfect-information games. arXiv
preprint arXiv:1603.01121, 2016.
Hoda, S., Gilpin, A., Pe  ̃na, J., and Sandholm, T. Smoothing
techniques for computing Nash equilibria of sequential
games. Mathematics of Operations Research, 35(2):494–
512, 2010. Conference version appeared in WINE-07.
Jackson, E. Targeted CFR. In AAAI Workshop on Computer
Poker and Imperfect Information, 2017.
Jackson, E. G. Compact CFR. In AAAI Workshop on Com-
puter Poker and Imperfect Information, 2016.
Jin, P. H., Levine, S., and Keutzer, K. Regret minimiza-
tion for partially observable deep reinforcement learning.
arXiv preprint arXiv:1710.11424, 2017.
Johanson, M., Bard, N., Lanctot, M., Gibson, R., and
Bowling, M. Efficient nash equilibrium approximation
through monte carlo counterfactual regret minimization.
In Proceedings of the 11th International Conference on
Autonomous Agents and Multiagent Systems-Volume 2,
pp. 837–846. International Foundation for Autonomous
Agents and Multiagent Systems, 2012.
Johanson, M., Burch, N., Valenzano, R., and Bowling,
M. Evaluating state-space abstractions in extensive-form
games. In Proceedings of the 2013 International Con-
ference on Autonomous Agents and Multiagent Systems,
pp. 271–278. International Foundation for Autonomous
Agents and Multiagent Systems, 2013.
Johanson, M. B. Robust Strategies and Counter-Strategies:
From Superhuman to Optimal Play. PhD thesis, Univer-
sity of Alberta, 2016.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
Kroer, C., Farina, G., and Sandholm, T. Solving large
sequential games with the excessive gap technique. In
Deep Counterfactual Regret Minimization
Deep CFR algorithm. This convergence bound is rather obvious and the derivation pedantic, so the reader is welcome to
skip this section.
We model T rounds of K-external sampling as T ×K rounds of external sampling, where at each round t ·K + d (for
integer t ≥0 and integer 0 ≤d < K) we play
σtK+d(a) =


R+
tK(a)
R+
Σ,tK
if R+
Σ,tK > 0
arbitrary, otherwise
(12)
In prior work, σ is typically defined to play 1
|A| when R+
Σ,T(a) ≤0, but in fact the convergence bounds do not constraint σ’s
play in these situations, which we will demonstrate explicitly here. We need this fact because minimizing the loss L(V ) is
defined only over the samples of (visited) infosets and thus does not constrain the strategy in unvisited infosets.
Lemma 2. If regret matching is used in K-ES, then for 0 ≤d < K
∑
a∈A
R+
tK(a)rtK+d(a) ≤0 (13)
Proof. If R+
Σ,tK ≤0, then R+
tK(a) = 0 for all a and the result follows directly. For R+
Σ,tK > 0,
∑
a∈A
R+
tK(a)rtK+d(a) = ∑
a∈A
R+
T (a)(utK+d(a) −utK+d(σtK)) (14)
=
(∑
a∈A
R+
tK(a)utK+d(a)
)
−
(
utK+d(σtK) ∑
a∈A
R+
tK(a)
)
(15)
=
(∑
a∈A
R+
tK(a)utK+d(a)
)
−
(∑
a∈A
σtK+d(a)utK+d(a)
)
R+
Σ,tK(a) (16)
=
(∑
a∈A
R+
tK(a)utK+d(a)
)
−
(∑
a∈A
R+
tK(a)
R+
Σ,tK(a) utK+d(a)
)
R+
Σ,tK(a) (17)
=
(∑
a∈A
R+
tK(a)utK+d(a)
)
−
(∑
a∈A
R+
tK(a)(a)utK+d(a)
)
(18)
= 0 (19)
Theorem 3. Playing according to Equation 12 guarantees the following bound on total regret
∑
a∈A
(R+
TK(a))2 ≤|A|∆2K2T (20)
Proof. We prove by recursion on T.
∑
a∈A
(R+
TK(a))2 ≤∑
a∈A
(
R+
(T−1)K(a) +
K−1∑
d=0
rtK−d(a)
)2
(21)
= ∑
a∈A
(
R+
(T−1)K(a)2 + 2
K−1∑
d=0
rd(a)R+
(T−1)K(a) +
K−1∑
d=0
K−1∑
d′=0
rTK−d(a)rTK−d′(a)
)
(22)
By Lemma 2,
Deep Counterfactual Regret Minimization
∑
a∈A
(R+
TK(a))2 ≤ ∑
a∈A
(R+
(T−1)K(a))2 + ∑
a∈A
K−1∑
d=0
K−1∑
d′=0
rTK−d(a)rTK−d′(a) (23)
By induction, ∑
a∈A
(R+
(T−1)K(a))2 ≤|A|∆2(T −1) (24)
From the definition, |rTK−d(a)|≤∆
∑
a∈A
(R+
TK(a))2 ≤|A|∆2(T −1) + K2|A|∆2 = |A|∆2K2T (25)
Theorem 4. (Lanctot 2013, Theorem 3 & Theorem 5) After T iterations of K-ES, average regret is bounded by
 ̄RTKp ≤
(
1 +
√2√ρK
)
|Ip|∆
√|A|√T (26)
with probability 1 −ρ.
Proof. The proof follows Lanctot 2013, Theorem 3. Note that K-ES is only different from ES in terms of the choice of
σT, and the proof in Lanctot 2013 only makes use of σT via the bound on (∑
a RT+(a))2 that we showed in Theorem 3.
Therefore, we can apply the same reasoning to arrive at
 ̃RTKp ≤ ∆Mp
√|A|TK
δ (27)
(Lanctot 2013, Eq. (4.30)).
Lanctot et al. 2009 then shows that  ̃RTKp and RTKp are similar with high probability, leading to
E



∑
I∈Ip
(RTKp (I) −  ̃RTKp (I))


2
 ≤ 2|Ip||Bp||A|TK∆2
δ2 (28)
(Lanctot 2013, Eq. (4.33), substituting T →TK).
Therefore, by Markov’s inequality, with probability at least 1 −ρ,
RTKp ≤
√2|Ip||Bp||A|TK∆
δ√ρ + ∆M√|A|TK
δ (29)
, where external sampling permits δ = 1 (Lanctot, 2013).
Using the fact that M ≤|Ip|and |Bp|< |Ip|and dividing through by KT leads to the simplified form
 ̄RTKp ≤
(
1 +
√2√ρK
)
∆|Ip|
√|A|√T (30)
with probability 1 −ρ.
